import java.io.File

import org.apache.spark.SparkContext
import org.apache.spark.sql.expressions.{Window, WindowSpec}
import org.apache.spark.sql.{Column, Row, SparkSession}

import scala.reflect.runtime.universe._
import org.apache.spark.sql.functions._
/**
  *
  * author: jomei
  * date: 2019/2/14 18:02
  */
object UBM_backup {
  def main(args: Array[String]): Unit = {
    //====================================================================
    //1)connect to cluster
    val warehouseLocation = new File("spark-warehouse").getAbsolutePath
    val spark = SparkSession
      .builder()
      .appName("Click Model")
      .config("spark.sql.warehouse.dir", warehouseLocation)
      .enableHiveSupport()
      .getOrCreate()
    import spark.implicits._
    val sc: SparkContext = spark.sparkContext
    spark.sparkContext.setLogLevel("ERROR")

    //====================================================================
    //2)initial variable
    val date_start = args(0)
    val date_end = args(1)
    val bayes = args(2).toInt
    val datatable = args(3)
    val thisdatatable = args(4)
    val threshold = args(5).toDouble

    println("parameter:")
    println ("date_start:" + date_start)
    println ("date_end:" + date_end)
    println ("bayes:" + bayes)
    println ("datatable:" + datatable)
    println ("thisdatatable:" + thisdatatable)
    println ("threshold:" + threshold)

    //====================================================================
    //3)acquire the data between date_start and date_end of datatable_song
    val sql_song_read= "select q, u, r, d, c, s, cnt, alpha_numerator, alpha_denominator, gamma_numerator, gamma_denominator, choric_singer, songname from "+s"$datatable"+"_song where cdt between "+s"'$date_start' and '$date_end'"
    val df_song_read = spark.sql(sql_song_read)
    df_song_read.persist()
    val min_alpha = df_song_read.groupBy("q", "u", "choric_singer", "songname").agg(sum("alpha_numerator").alias("numerator"), sum("alpha_denominator").alias("denominator")).withColumn("alpha", $"numerator"/$"denominator").agg(min("alpha")).first.getDouble(0)
    var mean_nums = threshold
    if (mean_nums==0){
      mean_nums = df_song_read.groupBy("q", "u", "choric_singer", "songname").agg(sum("alpha_numerator").alias("numerator"), sum("alpha_denominator").alias("denominator")).orderBy($"numerator".desc).limit(bayes).agg(mean("numerator")).first.getDouble(0)
    }
    //val mean_nums = df_song_read.groupBy("q", "u", "choric_singer", "songname").agg(sum("alpha_numerator").alias("numerator"), sum("alpha_denominator").alias("denominator")).orderBy($"denominator".desc).limit(bayes).agg(mean("denominator")).first.getDouble(0)
    val df_result = df_song_read.groupBy("q", "u", "choric_singer", "songname").agg(sum("alpha_numerator").alias("numerator"), sum("alpha_denominator").alias("denominator")).withColumn("alpha", (lit(mean_nums*min_alpha) + $"numerator")/(lit(mean_nums) + $"denominator"))

    df_result.createOrReplaceTempView("result_data")

    val sql_result_create= """
create table if not exists """+s"""$thisdatatable"""+"""_result
(
q string,
u string,
choric_singer string,
songname string,
numerator double,
denominator double,
alpha double
)
partitioned by (cdt string)
row format delimited fields terminated by '|' lines terminated by '\n' stored as textfile
"""

    spark.sql(sql_result_create)

    val sql_result_save= s"""
INSERT OVERWRITE TABLE """+s"""$thisdatatable"""+s"""_result PARTITION(cdt='$date_end') select q, u, choric_singer, songname, numerator, denominator, alpha from result_data
"""

    spark.sql(sql_result_save)

    //position donot deal with s === 1, cause s =!= 1 we return 0
    //for the position, we use the overall to count mean
    val sumNum =  df_song_read.agg(sum("gamma_numerator")).first.getDouble(0)
    val sumDen =  df_song_read.agg(sum("gamma_denominator")).first.getDouble(0)
    val df_position = df_song_read.groupBy("r", "d").agg(sum("gamma_numerator").alias("numerator"), sum("gamma_denominator").alias("denominator")).withColumn("gamma", (lit(mean_nums*sumNum/sumDen) + $"numerator")/(lit(mean_nums) + $"denominator"))

    df_position.createOrReplaceTempView("position_data")

    val sql_position_create= """
create table if not exists """+s"""$thisdatatable"""+"""_position
(
r string,
d string,
numerator double,
denominator double,
gamma double
)
partitioned by (cdt string)
row format delimited fields terminated by '|' lines terminated by '\n' stored as textfile
"""

    spark.sql(sql_position_create)

    val sql_position_save= s"""
INSERT OVERWRITE TABLE """+s"""$thisdatatable"""+s"""_position PARTITION(cdt='$date_end') select r, d, numerator, denominator, gamma from position_data
"""

    spark.sql(sql_position_save)

    //====================================================================
    //10)end
    spark.stop() //to avoid ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate

  }
}
